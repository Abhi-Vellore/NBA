{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Awards and Betting Models\n",
    "Part 1: Webscraping\n",
    "\n",
    "Author: Abhi Vellore\n",
    "Inspired By: Dataquest Web Scraping NBA Stats With Python: Data Project [Part 1 of 3] and https://github.com/JustinGong03/nba-awards-predictor\n",
    "\n",
    "Some portions are adapted from the below sources. \n",
    "https://www.youtube.com/watch?v=JGQGd-oa0l4\n",
    "https://github.com/JustinGong03/nba-awards-predictor\n",
    "Accessed 2023. \n",
    "\n",
    "Part 1 consists of using web scraping methods in order to gather data to eventually build our models. This part uses BeautifulSoup, Requests, and Selenium in order to create CSVs of important NBA statistics. These statistics will be used in the future to create models to predict winners of major NBA awards and build betting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tools\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MVP Data\n",
    "\n",
    "All of our data will be scraped from Basketball Reference, and we will initially focus on the MVP data, which is at the top of the awards page. \n",
    "\n",
    "We will use the Requests Module to save the html file in the \"mvp\" folder. After, we will use BeautifulSoup and Pandas to parse the data into a CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable of years to reference\n",
    "years = range(2000,2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to award winners\n",
    "url_start = \"https://www.basketball-reference.com/awards/awards_{}.html\"\n",
    "\n",
    "for year in years:\n",
    "   # Sleep to prevent being rate limited\n",
    "   time.sleep(2)\n",
    "   url = url_start.format(year)\n",
    "   data = requests.get(url)\n",
    "   with open(\"data/mvp/{}.html\".format(year), \"w+\") as f:\n",
    "      f.write(data.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of DataFrames \n",
    "dfsm = []\n",
    "\n",
    "for year in years:\n",
    "   # Open page to be read\n",
    "   with open (\"data/mvp/{}.html\".format(year)) as f:\n",
    "      page = f.read()\n",
    "   soup = BeautifulSoup(page, \"html.parser\")\n",
    "   # Rid of the header in the MVP table\n",
    "   soup.find(\"tr\", class_=\"over_header\").decompose()\n",
    "\n",
    "   # Build MVP table from HTML; append year\n",
    "   mvp_table = soup.find(id = \"mvp\")\n",
    "   mvp = pd.read_html(str(mvp_table))[0]\n",
    "   mvp[\"Year\"] = year\n",
    "   dfsm.append(mvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from list of dataframes into one CSV\n",
    "mvp = pd.concat(dfsm)\n",
    "mvp.to_csv(\"data/mvps.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Awards Data\n",
    "\n",
    "Unfortunately, Basketball Reference prevents us from accessing the other award information directly. From the same webpage, we therefore need to use Selenium instead.\n",
    "\n",
    "\n",
    "Selenium uses a Chrome Webdriver in order to emulate a Chrome browser, and \"scrolls\" through the site in order to load new information that is previously hidden. We can then similarly store this information as a HTML file and then parse it, once again using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium Webdriver. The latest version of Selenium automatically sets up a driver without additional input\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping whole web page. Note this can be combined with the MVP scrape, but is done separately\n",
    "# as a learning experience.\n",
    "\n",
    "for year in years:\n",
    "   url = \"https://www.basketball-reference.com/awards/awards_{}.html\".format(year)\n",
    "   driver.get(url) \n",
    "   # Simulate the \"scroll\" using javascript\n",
    "   driver.execute_script(\"window.scrollTo(1, 10000)\")\n",
    "   time.sleep(2)\n",
    "\n",
    "   html = driver.page_source\n",
    "\n",
    "   # Store data\n",
    "   with open(\"data/otherAwards/{}.html\".format(year), \"w+\") as f:\n",
    "      f.write(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of dataframes to later save as CSV\n",
    "dfsd = []\n",
    "dfss = []\n",
    "dfsmi = []\n",
    "\n",
    "for year in years:\n",
    "\n",
    "   # Open page to be read\n",
    "   with open (\"data/otherAwards/{}.html\".format(year)) as f:\n",
    "      page = f.read()\n",
    "   soup = BeautifulSoup(page, \"html.parser\")\n",
    "   # Rid of all headers in tables\n",
    "   for header in soup.find_all(\"tr\", class_=\"over_header\"):\n",
    "      header.decompose()\n",
    "\n",
    "   # DPOY Table\n",
    "   dpoy_table = soup.find(id = \"dpoy\")\n",
    "   dpoy = pd.read_html(str(dpoy_table))[0]\n",
    "   dpoy[\"Year\"] = year\n",
    "   dfsd.append(dpoy)\n",
    "\n",
    "   # SMOY Table\n",
    "   smoy_table = soup.find(id = \"smoy\")\n",
    "   smoy = pd.read_html(str(smoy_table))[0]\n",
    "   smoy[\"Year\"] = year\n",
    "   dfss.append(smoy)\n",
    "\n",
    "   # MIP Table\n",
    "   mip_table = soup.find(id = \"mip\")\n",
    "   mip = pd.read_html(str(mip_table))[0]\n",
    "   mip[\"Year\"] = year\n",
    "   dfsmi.append(mip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine DFs and Convert to CSV\n",
    "\n",
    "dpoy = pd.concat(dfsd)\n",
    "smoy = pd.concat(dfss)\n",
    "mip = pd.concat(dfsmi)\n",
    "dpoy.to_csv(\"data/dpoys.csv\")\n",
    "smoy.to_csv(\"data/smoys.csv\")\n",
    "mip.to_csv(\"data/mips.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Player Statistics \n",
    "\n",
    "So far, we have gathered information about award winners. However, we must also know how award winners stand relative to all other players, so we scrape information about all players across the year range. Again, we use Selenium in order to generate the full table.\n",
    "\n",
    "Note: we only want to consider players that have played a minimum amount of games to rid of extraneous data. We choose 28 games as a minimum, or 1/3 of the season. Also, players can get traded during the season. To avoid predictions, we will pre-clean the data to drop such instances/players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\"\n",
    "\n",
    "for year in years:\n",
    "   url = player_stats_url.format(year)\n",
    "   driver.get(url)\n",
    "   # Simulate the \"scroll\" using javascript\n",
    "   driver.execute_script(\"window.scrollTo(1, 10000)\")\n",
    "   time.sleep(2)\n",
    "\n",
    "   html = driver.page_source\n",
    "\n",
    "   # Store the data\n",
    "   with open(\"data/player/{}.html\".format(year), \"w+\") as f:\n",
    "      f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of dataframes to later save as CSV\n",
    "dfPerGame = []\n",
    "\n",
    "for year in years:\n",
    "   # Open page to be read\n",
    "   with open (\"data/player/{}.html\".format(year)) as f:\n",
    "      page = f.read()\n",
    "   soup = BeautifulSoup(page, \"html.parser\")\n",
    "   # Rid of all headers in tables\n",
    "   for header in soup.find_all(\"tr\", class_=\"thead\"):\n",
    "      header.decompose()\n",
    "\n",
    "   # Player table\n",
    "   player_table = soup.find(id = \"per_game_stats\")\n",
    "   players = pd.read_html(str(player_table))[0]\n",
    "   players[\"Year\"] = year\n",
    "   dfPerGame.append(players)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Player 36 Minute Stats + Advanced Stats\n",
    "\n",
    "In order to improve our models, it's critical to also have access to advanced statistics and per 36 minute statistics, to account for differences in playing time and team situations. We'll combine both the advanced statistics and per 36 minutes into one table per player. \n",
    "\n",
    "NOTE: Now that we're comfortable using Selenium, we'll directly add the data into a list of dfs without saving the html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsPer36 = []\n",
    "dfsAdv = []\n",
    "\n",
    "for year, type_ in product(years, [\"per_poss\", \"advanced\"]):\n",
    "      driver.get(\"https://www.basketball-reference.com/leagues/NBA_{}_{}.html\".format(year, type_))\n",
    "      driver.execute_script(\"window.scrollTo(1, 20000)\")\n",
    "      time.sleep(2)\n",
    "      html = driver.page_source\n",
    "      \n",
    "      soup = BeautifulSoup(html, \"html.parser\")\n",
    "      for header in soup.find_all(\"tr\", class_=\"thead\"):\n",
    "            header.decompose()\n",
    "\n",
    "      data = soup.find(id = \"{}_stats\".format(type_))\n",
    "      df = pd.read_html(str(data))[0]\n",
    "      df[\"Year\"] = year\n",
    "      if type_ == \"per_poss\":\n",
    "            dfsPer36.append(df)\n",
    "      else:\n",
    "            dfsAdv.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine DFs and Convert to one CSV with all player statistics\n",
    "\n",
    "playersPerGame = pd.concat(dfPerGame)\n",
    "playersPer36 = pd.concat(dfsPer36)\n",
    "playersAdv = pd.concat(dfsAdv)\n",
    "\n",
    "playersPerGame.to_csv(\"data/players.csv\")\n",
    "playersPer36.to_csv(\"data/players36.csv\")\n",
    "playersAdv.to_csv(\"data/playersAdv.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Data Statistics\n",
    "\n",
    "Next, team data is also critical and plays a role in award winners, and of course provides information about winning teams. While we can just use the requests module,  \n",
    "\n",
    "Similar to how we did with players, we should collect advanced informations about the performance of each term for our later betting model and to help provide context for player statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link to team standings\n",
    "url_start = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\"\n",
    "\n",
    "for year in years:\n",
    "   time.sleep(2)\n",
    "   url = url_start.format(year)\n",
    "   data = requests.get(url)\n",
    "   with open(\"data/teams/{}.html\".format(year), \"w+\") as f:\n",
    "      f.write(data.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We use Selenium to scrape all our team information. However, the \"click\" function works better when we are not also scrolling, hence, there may occasionally be bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of dataframes to later save as CSV\n",
    "dfst = []\n",
    "dfso = []\n",
    "dfsa = []\n",
    "dfss = []\n",
    "\n",
    "for year in years:\n",
    "      \n",
    "      driver.get(\"https://www.basketball-reference.com/leagues/NBA_{}.html\".format(year))\n",
    "      if year < 2016:\n",
    "            driver.execute_script(\"window.scrollTo(1, 2300)\")\n",
    "      else:\n",
    "            driver.execute_script(\"window.scrollTo(1, 2700)\") \n",
    "      time.sleep(2)\n",
    "      html = driver.page_source\n",
    "      \n",
    "      soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "\n",
    "      # Remove all table overheaders\n",
    "      for header in soup.find_all(\"tr\", class_=\"over_header\"):\n",
    "            header.decompose()\n",
    "\n",
    "\n",
    "      # Per Game Team Stats\n",
    "      data = soup.find(id = \"per_game-team\")\n",
    "      df = pd.read_html(str(data))[0]\n",
    "      df[\"Year\"] = year\n",
    "      dfst.append(df)\n",
    "\n",
    "\n",
    "      # Scroll to the correct statistics and switch tabs  \n",
    "      opponent_tab = driver.find_element(\"link text\", \"Opponent\")\n",
    "      opponent_tab.click()\n",
    "      \n",
    "      # Wait for the \"Opponent\" tab content to load\n",
    "      time.sleep(3)\n",
    "      \n",
    "      # Get the page source with the \"Opponent\" tab content\n",
    "      opponent_page_source = driver.page_source\n",
    "      soup = BeautifulSoup(opponent_page_source, \"html.parser\")\n",
    "\n",
    "      # Remove all table overheaders in new website\n",
    "      for header in soup.find_all(\"tr\", class_=\"over_header\"):\n",
    "            header.decompose()\n",
    "\n",
    "      # Per game information on opponents\n",
    "      data = soup.find(id = \"per_game-opponent\")\n",
    "      df = pd.read_html(str(data))[0]\n",
    "      df[\"Year\"] = year\n",
    "      dfso.append(df)      \n",
    "\n",
    "      # Scroll further down\n",
    "      driver.execute_script(\"window.scrollTo(1, 7500)\")\n",
    "      time.sleep(2)\n",
    "      html = driver.page_source\n",
    "\n",
    "      # Advanced Stats\n",
    "\n",
    "      data = soup.find(id = \"advanced-team\")\n",
    "      df = pd.read_html(str(data))[0]\n",
    "      df[\"Year\"] = year\n",
    "      dfsa.append(df)\n",
    "\n",
    "      # Shooting Stats\n",
    "\n",
    "      data = soup.find(id = \"shooting-team\")\n",
    "      df = pd.read_html(str(data))[0]\n",
    "      df[\"Year\"] = year\n",
    "      dfss.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine DFs and Convert to CSV\n",
    "\n",
    "teamsAllStats = pd.concat(dfst)\n",
    "defenseStats = pd.concat(dfso)\n",
    "teamsAdvanced = pd.concat(dfsa)\n",
    "shootingTeam = pd.concat(dfss)\n",
    "\n",
    "teamsAllStats.to_csv(\"data/teamAllStats.csv\")\n",
    "teamsAdvanced.to_csv(\"data/teamAdvanced.csv\")\n",
    "defenseStats.to_csv(\"data/teamDefense.csv\")\n",
    "shootingTeam.to_csv(\"data/shooting.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4110492365f65d82c8da42554efff219ecd7e350ec190bde6db9989f21ebdc5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
